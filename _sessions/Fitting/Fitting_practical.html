<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Fitting</title>

<script src="Fitting_practical_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Fitting_practical_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Fitting_practical_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Fitting_practical_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Fitting_practical_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="Fitting_practical_files/navigation-1.1/tabsets.js"></script>
<link href="Fitting_practical_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Fitting_practical_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="practical.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Fitting</h1>
<h4 class="author"><table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'>
<col width='10%'>
<col width='10%'>
<tr style="border:none">
<td style="display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none" nowrap>
<font style='font-style:normal'>Machine Learning with R</font><br> <a href='https://therbootcamp.github.io/ML_2019Oct/'> <i class='fas fa-clock' style='font-size:.9em;' ></i> </a> <a href='https://therbootcamp.github.io'> <i class='fas fa-home' style='font-size:.9em;'></i> </a> <a href='mailto:therbootcamp@gmail.com'> <i class='fas fa-envelope' style='font-size: .9em;'></i> </a> <a href='https://www.linkedin.com/company/basel-r-bootcamp/'> <i class='fab fa-linkedin' style='font-size: .9em;'></i> </a> <a href='https://therbootcamp.github.io'> <font style='font-style:normal'>Basel R Bootcamp</font> </a>
</td>
<td style="width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none">
<img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
</td>
</tr>
</table></h4>

</div>


<p align="center">
<img width="100%" src="image/fitting_dirk.001.png" margin=0><br> <font style="font-size:10px">adapted from <a href="https://xkcd.com/">xkcd.com</a></font>
</p>
<div id="section" class="section level1 tabset">
<h1></h1>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>In this practical, you’ll practice the basics of fitting and exploring regression models in R.</p>
<p>By the end of this practical you will know how to:</p>
<ol style="list-style-type: decimal">
<li>Fit a regression model to training data.</li>
<li>Explore your fit object with generic functions.</li>
<li>Evaluate the model’s fitting performance using accuracy measures such as MSE and MAE.</li>
<li>Explore the effects of adding additional features.</li>
</ol>
</div>
<div id="tasks" class="section level2">
<h2>Tasks</h2>
<div id="a---setup" class="section level3">
<h3>A - Setup</h3>
<ol style="list-style-type: decimal">
<li>Open your <code>BaselRBootcamp</code> R project. It should already have the folders <code>1_Data</code> and <code>2_Code</code>. Make sure that the data file(s) listed in the <code>Datasets</code> section are in your <code>1_Data</code> folder</li>
</ol>
<pre class="r"><code># Done!</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Open a new R script. At the top of the script, using comments, write your name and the date. Save it as a new file called <code>Fitting_practical.R</code> in the <code>2_Code</code> folder.</li>
</ol>
<pre class="r"><code># Done!</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Using <code>library()</code> load the set of packages for this practical listed in the packages section above.</li>
</ol>
<pre class="r"><code># Load packages necessary for this script
library(tidyverse)
library(caret)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>For this practical, we’ll use a dataset of 388 U.S. Colleges. The data is stored in <code>college_train.csv</code>. Using the following template, load the dataset into R as <code>college_train</code>:</li>
</ol>
<pre class="r"><code># Load in college_train.csv data as college_train
college_train &lt;- read_csv(file = &quot;1_Data/college_train.csv&quot;)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Take a look at the first few rows of the dataset by printing it to the console.</li>
</ol>
<pre class="r"><code>college_train</code></pre>
<pre><code># A tibble: 500 x 18
   Private  Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad
   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
 1 Yes      1202   1054    326        18        44        1410         299
 2 Yes      1415    714    338        18        52        1345          44
 3 Yes      4778   2767    678        50        89        2587         120
 4 Yes      1220    974    481        28        67        1964         623
 5 Yes      1981   1541    514        18        36        1927        1084
 6 Yes      1217   1088    496        36        69        1773         884
 7 No       8579   5561   3681        25        50       17880        1673
 8 No        833    669    279         3        13        1224         345
 9 No      10706   7219   2397        12        37       14826        1979
10 Yes       938    864    511        29        62        1715         103
# … with 490 more rows, and 10 more variables: Outstate &lt;dbl&gt;,
#   Room.Board &lt;dbl&gt;, Books &lt;dbl&gt;, Personal &lt;dbl&gt;, PhD &lt;dbl&gt;,
#   Terminal &lt;dbl&gt;, S.F.Ratio &lt;dbl&gt;, perc.alumni &lt;dbl&gt;, Expend &lt;dbl&gt;,
#   Grad.Rate &lt;dbl&gt;</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Print the numbers of rows and columns using the <code>dim()</code> function.</li>
</ol>
<pre class="r"><code># Print number of rows and columns of college_train
dim(XXX)</code></pre>
<pre class="r"><code># Print number of rows and columns of college_train
dim(college_train)</code></pre>
<pre><code>[1] 500  18</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Look at the names of the dataframe with the <code>names()</code> function</li>
</ol>
<pre class="r"><code>names(XXX)</code></pre>
<pre class="r"><code>names(college_train)</code></pre>
<pre><code> [1] &quot;Private&quot;     &quot;Apps&quot;        &quot;Accept&quot;      &quot;Enroll&quot;      &quot;Top10perc&quot;  
 [6] &quot;Top25perc&quot;   &quot;F.Undergrad&quot; &quot;P.Undergrad&quot; &quot;Outstate&quot;    &quot;Room.Board&quot; 
[11] &quot;Books&quot;       &quot;Personal&quot;    &quot;PhD&quot;         &quot;Terminal&quot;    &quot;S.F.Ratio&quot;  
[16] &quot;perc.alumni&quot; &quot;Expend&quot;      &quot;Grad.Rate&quot;  </code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Open the dataset in a new window using <code>View()</code>. How does it look?</li>
</ol>
<pre class="r"><code>View(XXX)</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>Before starting to model the data, we need to do a little bit of data cleaning. Specifically, we need to convert all character columns to factors: Do this by running the following code:</li>
</ol>
<pre class="r"><code># Convert character to factor
college_train &lt;- college_train %&gt;%
          mutate_if(is.character, factor)</code></pre>
</div>
<div id="b---determine-sampling-procedure" class="section level3">
<h3>B - Determine sampling procedure</h3>
<p>In <code>caret</code>, we define the computational nuances of training using the <code>trainControl()</code> function. Because we’re learning the basics of fitting, we’ll set <code>method = &quot;none&quot;</code> for now. (Note that you would almost never do this for a real prediction task, you’ll see why later!)</p>
<pre class="r"><code># Set training resampling method to &quot;none&quot; to keep everything super simple
#  for demonstration purposes. Note that you would almost never
#  do this for a real prediction task!
ctrl_none &lt;- trainControl(method = &quot;none&quot;) </code></pre>
</div>
<div id="regression" class="section level3">
<h3>Regression</h3>
</div>
<div id="c---fit-a-regression-model" class="section level3">
<h3>C - Fit a regression model</h3>
<ol style="list-style-type: decimal">
<li>Using the code below, fit a regression model predicting graduation rate (<code>Grad.Rate</code>) as a function of one feature <code>PhD</code> (percent of faculty with PhDs). Save the result as an object <code>Grad.Rate_glm</code>. Specifically,…</li>
</ol>
<ul>
<li>set the <code>form</code> argument to <code>Grad.Rate ~ PhD</code>.</li>
<li>set the <code>data</code> argument to your training data <code>college_train</code>.</li>
<li>set the <code>method</code> argument to <code>&quot;glm&quot;</code> for regression.</li>
<li>set the <code>trControl</code> argument to <code>ctrl_none</code>, the object you created previously.</li>
</ul>
<pre class="r"><code># Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD
Grad.Rate_glm &lt;- train(form = XX ~ XX,
                       data = XX,
                       method = &quot;XX&quot;,
                       trControl = XX)</code></pre>
<pre class="r"><code># Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD
Grad.Rate_glm &lt;- train(form = Grad.Rate ~ PhD,
                       data = college_train,
                       method = &quot;glm&quot;,
                       trControl = ctrl_none)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Explore the fitted model using the <code>summary()</code> function, by setting the function’s first argument to <code>Grad.Rate_glm</code>.</li>
</ol>
<pre class="r"><code># Show summary information from the regression model
summary(XXX)</code></pre>
<pre class="r"><code># Show summary information from the regression model
summary(Grad.Rate_glm)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-43.83  -10.44    0.49   10.93   41.47  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   41.372      3.382   12.23  &lt; 2e-16 ***
PhD            0.330      0.045    7.33  9.1e-13 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for gaussian family taken to be 257)

    Null deviance: 141641  on 499  degrees of freedom
Residual deviance: 127832  on 498  degrees of freedom
AIC: 4197

Number of Fisher Scoring iterations: 2</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Look at the results, how do you interpret the regression coefficients? What is the general relationship between PhD and graduation rates? Does this make sense?</li>
</ol>
<pre class="r"><code># For every increase of one in PhD (the percent of faculty with a PhD), the expected graduation rate increases by 0.33</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Now it’s time to save the model’s fitted values! Do this by running the following code to save the fitted values as <code>glm_fit</code>. Tip: Set the first argument to <code>Grad.Rate_glm</code>.</li>
</ol>
<pre class="r"><code># Get fitted values from the Grad.Rate_glm model and save as glm_fit
glm_fit &lt;- predict(XXX)</code></pre>
<pre class="r"><code># Get fitted values from the model and save as glm_fit
glm_fit &lt;- predict(Grad.Rate_glm)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Plot the distribution of your <code>glm_fit</code> object using the code below - what are these values? Do they look reasonable? That is, are they in the range of what you expect the criterion to be?</li>
</ol>
<pre class="r"><code># Plot glm_fit
hist(glm_fit)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-22-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Yes, values appear to be within 40 and 80, which is what we expect from the truth population.</code></pre>
</div>
<div id="d---evaluate-accuracy" class="section level3">
<h3>D - Evaluate accuracy</h3>
<ol style="list-style-type: decimal">
<li>Now it’s time to compare your model fits to the true values. We’ll start by defining the vector <code>criterion</code> as the actual graduation rates.</li>
</ol>
<pre class="r"><code># Define criterion as Grad.Rate
criterion &lt;- college_train$Grad.Rate</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Let’s quantify our model’s fitting results. To do this, we’ll use the <code>postResample()</code> function, with the fitted values as the prediction, and the criterion as the observed values.</li>
</ol>
<p>Specifically,</p>
<ul>
<li>set the <code>pred</code> argument to <code>glm_fit</code> (your fitted values).</li>
<li>set the <code>obs</code> argument to <code>criterion</code> (a vector of the criterion values).</li>
</ul>
<pre class="r"><code># Regression Fitting Accuracy
postResample(pred = XXX,   # Fitted values 
             obs = XXX)  # criterion values</code></pre>
<pre class="r"><code># Regression Fitting Accuracy
postResample(pred = glm_fit,   # Fitted values 
             obs = criterion)  # criterion values</code></pre>
<pre><code>    RMSE Rsquared      MAE 
 15.9895   0.0975  12.8633 </code></pre>
<ol start="3" style="list-style-type: decimal">
<li>You’ll see three values here. The easiest to understand is MAE which stands for “Mean Absolute Error” – in other words, “on average how far are the predictions from the true values?” A value of 0 means perfect prediction, so small values are good! How do you interpret these results?</li>
</ol>
<pre class="r"><code># On average, the model fits are 12.8633 away from the true values.
#  Whether this is &#39;good&#39; or not depends on you :)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Now we’re ready to do some plotting. But first, we need to re-organise the data a bit. We’ll create two dataframes:</li>
</ol>
<ul>
<li><code>accuracy</code> - Raw absolute errors</li>
<li><code>accuracy_agg</code> - Aggregate (i.e.; mean) absolute errors</li>
</ul>
<pre class="r"><code># accuracy - a dataframe of raw absolute errors
accuracy &lt;- tibble(criterion = criterion,
                   Regression = glm_fit) %&gt;%
                gather(model, prediction, -criterion) %&gt;%
                # Add error measures
                mutate(ae = abs(prediction - criterion))

# accuracy_agg - Dataframe of aggregate errors
accuracy_agg &lt;- accuracy %&gt;%
                  group_by(model) %&gt;%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Print the <code>accuracy</code> and <code>accuracy_agg</code> objects to see how they look.</li>
</ol>
<pre class="r"><code>accuracy
accuracy_agg</code></pre>
<pre class="r"><code>head(accuracy) # Just printing the first few rows</code></pre>
<pre><code># A tibble: 6 x 4
  criterion model      prediction    ae
      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;
1        65 Regression       67.1  2.12
2        69 Regression       58.5 10.5 
3        83 Regression       66.8 16.2 
4        49 Regression       61.5 12.5 
5        80 Regression       65.5 14.5 
6        67 Regression       52.9 14.1 </code></pre>
<pre class="r"><code>head(accuracy_agg)</code></pre>
<pre><code># A tibble: 1 x 2
  model        mae
  &lt;chr&gt;      &lt;dbl&gt;
1 Regression  12.9</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Using the code below, create a scatterplot showing the relationship between the true criterion values and the model fits.</li>
</ol>
<pre class="r"><code># Plot A) Scatterplot of criterion versus predictions
ggplot(data = accuracy,
       aes(x = criterion, y = prediction)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = &quot;Regression: One Feature&quot;,
       subtitle = &quot;Line indicates perfect performance&quot;,
       x = &quot;True Graduation Rates&quot;,
       y = &quot;Fitted Graduation Rates&quot;) +
  xlim(0, 120) + 
  ylim(0, 120)</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>Look at the plot, how do you interpret this? Do you think the model did well or not in fitting the graduation rates?</li>
</ol>
<pre class="r"><code># No the model is not great, values do not fall very closely to the black diagonal line.</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>Let’s create a new violin plot showing the distribution of absolute errors of the model.</li>
</ol>
<pre class="r"><code># Plot B) Violin plot of absolute errors
ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = &quot;Distributions of Fitting Absolute Errors&quot;,
       subtitle = &quot;Numbers indicate means&quot;,
       x = &quot;Model&quot;,
       y = &quot;Absolute Error&quot;) +
  guides(fill = FALSE) +
  annotate(geom = &quot;label&quot;, 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>What does the plot show you about the model fits? On average, how far away were the model fits from the true values?</li>
</ol>
<pre class="r"><code># On average, the model fits are 12.86 away from the true criterion values.
#  However, there is also quite a bit of variability</code></pre>
</div>
<div id="e---add-more-features" class="section level3">
<h3>E - Add more features</h3>
<p>So far we have only used one feature (<code>PhD</code>), to predict <code>Grad.Rate</code>. Let’s try again, but now we’ll use a total of four features:</p>
<ul>
<li><code>PhD</code> - the percent of faculty with a PhD.</li>
<li><code>Room.Board</code> - room and board costs.</li>
<li><code>Terminal</code> - percent of faculty with a terminal degree.</li>
<li><code>S.F.Ratio</code> - student to faculty ratio.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Using the same steps as above, create a regression model <code>Grad.Rate_glm</code> which predicts <code>Grad.Rate</code> using all 4 features (you can also call it something else if you want to save your original model!). Specifically,…</li>
</ol>
<ul>
<li>set the <code>form</code> argument to <code>Grad.Rate ~ PhD + Room.Board + Terminal + S.F.Ratio</code>.</li>
<li>set the <code>data</code> argument to your training data <code>college_train</code>.</li>
<li>set the <code>method</code> argument to <code>&quot;glm&quot;</code> for regression.</li>
<li>set the <code>trControl</code> argument to <code>ctrl_none</code>.</li>
</ul>
<pre class="r"><code># Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD, Room.Board, Terminal, S.F.Ratio
Grad.Rate_glm &lt;- train(form = XXX ~ XXX + XXX + XXX + XXX,
                       data = XXX,
                       method = &quot;XXX&quot;,
                       trControl = XXX)</code></pre>
<pre class="r"><code># Grad.Rate_glm: Regression Model
#   Criterion: Grad.Rate
#   Features: PhD, Room.Board, Terminal, S.F.Ratio
Grad.Rate_glm &lt;- train(form = Grad.Rate ~ PhD + Room.Board + Terminal + S.F.Ratio,
                       data = college_train,
                       method = &quot;glm&quot;,
                       trControl = ctrl_none)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Explore your model using <code>summary()</code>. Which features seem to be important? Tip: set the first argument to <code>Grad.Rate_glm</code>.</li>
</ol>
<pre class="r"><code>summary(XXX)</code></pre>
<pre class="r"><code>summary(Grad.Rate_glm)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-45.10   -9.63    0.40   10.07   48.55  

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.635042   5.288467    7.31  1.1e-12 ***
PhD          0.217725   0.080744    2.70   0.0072 ** 
Room.Board   0.004674   0.000676    6.91  1.5e-11 ***
Terminal    -0.021957   0.088196   -0.25   0.8035    
S.F.Ratio   -0.524664   0.176980   -2.96   0.0032 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for gaussian family taken to be 224)

    Null deviance: 141641  on 499  degrees of freedom
Residual deviance: 110813  on 495  degrees of freedom
AIC: 4131

Number of Fisher Scoring iterations: 2</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Save the model’s fitted values as a new object <code>glm_fit</code>. I.e., set the first argument of <code>predict()</code> to your <code>Grad.Rate_glm</code> model.</li>
</ol>
<pre class="r"><code># Save new model fits
glm_fit &lt;- predict(XXX)</code></pre>
<pre class="r"><code># Save new model fits
glm_fit &lt;- predict(Grad.Rate_glm)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>By comparing the model fits to the true criterion values using <code>postResample()</code> calculate the Mean Absolute Error (MAE) of your new model that uses 4 features. How does this compare to your previous model that only used 1 feature? Specifically,…</li>
</ol>
<ul>
<li>set the <code>pred</code> argument to <code>glm_fit</code>, your model fits.</li>
<li>set the <code>obs</code> argument to <code>criterion</code>, a vector of the true criterion values.</li>
</ul>
<pre class="r"><code># New model fitting accuracy
postResample(pred = XXX,   # Fitted values 
             obs = XXX)  # criterion values</code></pre>
<pre class="r"><code># New model fitting accuracy
postResample(pred = glm_fit,   # Fitted values 
             obs = criterion)  # criterion values</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  14.887    0.218   11.779 </code></pre>
<pre class="r"><code># The new MAE value is 11.779, it&#39;s better (smaller) than the previous model, but still not great (in my opinion)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>(Optional). Create a scatter plot showing the relationship between your new model fits and the true values. How does this plot compare to your previous one?</li>
</ol>
<pre class="r"><code># accuracy: a dataframe of raw absolute errors
accuracy &lt;- tibble(criterion = criterion,
                   Regression = glm_fit) %&gt;%
                gather(model, prediction, -criterion) %&gt;%
  
  # Add error measures
  mutate(ae = abs(prediction - criterion))

# accuracy_agg: Dataframe of aggregate errors
accuracy_agg &lt;- accuracy %&gt;%
                  group_by(model) %&gt;%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of criterion versus predictions
ggplot(data = accuracy,
       aes(x = criterion, y = prediction)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = &quot;Regression: Four Features&quot;,
       subtitle = &quot;Line indicates perfect performance&quot;,
       x = &quot;True Graduation Rates&quot;,
       y = &quot;Fitted Graduation Rates&quot;) +
  xlim(0, 120) + 
  ylim(0, 120)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-44-1.png" width="60%" style="display: block; margin: auto;" /></p>
<ol start="6" style="list-style-type: decimal">
<li>(Optional). Create a violin plot showing the distribution of absolute errors. How does this compare to your previous one?</li>
</ol>
<pre class="r"><code># Plot B) Violin plot of absolute errors
ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = &quot;Distributions of Fitting Absolute Errors&quot;,
       subtitle = &quot;Numbers indicate means&quot;,
       x = &quot;Model&quot;,
       y = &quot;Absolute Error&quot;) +
  guides(fill = FALSE) +
  annotate(geom = &quot;label&quot;, 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-45-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="f---use-all-features" class="section level3">
<h3>F - Use all features</h3>
<p>Alright, now it’s time to use all features available!</p>
<ol style="list-style-type: decimal">
<li>Using the same steps as above, create a regression model <code>glm_fit</code> which predicts <code>Grad.Rate</code> using <em>all</em> features in the dataset. Specifically,…</li>
</ol>
<ul>
<li>set the <code>form</code> argument to <code>Grad.Rate ~ .</code>.</li>
<li>set the <code>data</code> argument to the training data <code>college_train</code>.</li>
<li>set the <code>method</code> argument to <code>&quot;glm&quot;</code> for regression.</li>
<li>set the <code>trControl</code> argument to <code>ctrl_none</code>.</li>
</ul>
<pre class="r"><code>Grad.Rate_glm &lt;- train(form = XXX ~ .,
                       data = XXX,
                       method = &quot;glm&quot;,
                       trControl = XXX)</code></pre>
<pre class="r"><code>Grad.Rate_glm &lt;- train(form = Grad.Rate ~ .,
                       data = college_train,
                       method = &quot;glm&quot;,
                       trControl = ctrl_none)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Explore your model using <code>summary()</code>, which features seem to be important?</li>
</ol>
<pre class="r"><code>summary(XXX)</code></pre>
<pre class="r"><code>summary(Grad.Rate_glm)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-38.10   -7.24   -0.58    7.51   47.10  

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 31.010972   5.911481    5.25  2.3e-07 ***
PrivateYes   1.701840   2.114677    0.80  0.42135    
Apps         0.001926   0.000572    3.37  0.00082 ***
Accept      -0.001754   0.001046   -1.68  0.09417 .  
Enroll       0.005550   0.002872    1.93  0.05387 .  
Top10perc   -0.049727   0.086281   -0.58  0.56466    
Top25perc    0.206252   0.066972    3.08  0.00219 ** 
F.Undergrad -0.001069   0.000461   -2.32  0.02068 *  
P.Undergrad -0.001294   0.000444   -2.92  0.00369 ** 
Outstate     0.001782   0.000297    6.01  3.7e-09 ***
Room.Board   0.000871   0.000721    1.21  0.22790    
Books       -0.000932   0.004089   -0.23  0.81988    
Personal    -0.001457   0.000998   -1.46  0.14494    
PhD          0.104743   0.071027    1.47  0.14095    
Terminal    -0.101789   0.076321   -1.33  0.18293    
S.F.Ratio    0.275943   0.191423    1.44  0.15008    
perc.alumni  0.219944   0.061576    3.57  0.00039 ***
Expend      -0.000683   0.000202   -3.39  0.00077 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for gaussian family taken to be 155)

    Null deviance: 141641  on 499  degrees of freedom
Residual deviance:  74595  on 482  degrees of freedom
AIC: 3960

Number of Fisher Scoring iterations: 2</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Save the model’s fitted values as a new object <code>glm_fit</code>.</li>
</ol>
<pre class="r"><code># Save new model fits
glm_fit &lt;- predict(XXX)</code></pre>
<pre class="r"><code># Save new model fits
glm_fit &lt;- predict(Grad.Rate_glm)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>What is the Mean Absolute Error (MAE) of your new model that uses 17 features? How does this compare to your previous model that only used 1 feature?</li>
</ol>
<pre class="r"><code># New model fitting accuracy
postResample(pred = glm_fit,   # Fitted values 
             obs = criterion)  # criterion values</code></pre>
<pre><code>    RMSE Rsquared      MAE 
  12.214    0.473    9.250 </code></pre>
<ol start="5" style="list-style-type: decimal">
<li>(Optional). Create a scatter plot showing the relationship between your new model fits and the true values. How does this plot compare to your previous one?</li>
</ol>
<pre class="r"><code># accuracy: a dataframe of raw absolute errors
accuracy &lt;- tibble(criterion = criterion,
                   Regression = glm_fit) %&gt;%
                gather(model, prediction, -criterion) %&gt;%
  
  # Add error measures
  mutate(ae = abs(prediction - criterion))

# accuracy_agg: Dataframe of aggregate errors
accuracy_agg &lt;- accuracy %&gt;%
                  group_by(model) %&gt;%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of criterion versus predictions

ggplot(data = accuracy,
       aes(x = criterion, y = prediction)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = &quot;Regression: All Features&quot;,
       subtitle = &quot;Line indicates perfect performance&quot;,
       x = &quot;True Graduation Rates&quot;,
       y = &quot;Fitted Graduation Rates&quot;) +
  xlim(0, 120) + 
  ylim(0, 120)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-53-1.png" width="60%" style="display: block; margin: auto;" /></p>
<ol start="6" style="list-style-type: decimal">
<li>(Optional). Create a violin plot showing the distribution of absolute errors. How does this compare to your previous one?</li>
</ol>
<pre class="r"><code># Plot B) Violin plot of absolute errors
ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = &quot;Distributions of Fitting Absolute Errors&quot;,
       subtitle = &quot;Numbers indicate means&quot;,
       x = &quot;Model&quot;,
       y = &quot;Absolute Error&quot;) +
  guides(fill = FALSE) +
  annotate(geom = &quot;label&quot;, 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-54-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="classification" class="section level3">
<h3>Classification</h3>
</div>
<div id="g---make-sure-your-criterion-is-a-factor" class="section level3">
<h3>G - Make sure your criterion is a factor!</h3>
<p>Now it’s time to do a classification task! Recall that in a classification task, we are predicting a category, not a continuous number. In this task, we’ll predict whether or not a college is Private or Public, this is stored as the variable <code>college_train$Private</code>.</p>
<ol style="list-style-type: decimal">
<li>In order to do classification training with <code>caret</code>, all you need to do is make sure that the criterion is coded as a factor. To test whether it is coded as a factor, you can look at its <code>class</code> as follows.</li>
</ol>
<pre class="r"><code># Look at the class of the variable Private, should be a factor!
class(college_train$Private)</code></pre>
<pre><code>[1] &quot;factor&quot;</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Now, we’ll save the Private column as a new object called <code>criterion</code>.</li>
</ol>
<pre class="r"><code># Define criterion as college_train$Private
criterion &lt;- college_train$Private</code></pre>
</div>
<div id="h---fit-a-classification-model" class="section level3">
<h3>H - Fit a classification model</h3>
<ol style="list-style-type: decimal">
<li>Using <code>train()</code>, create <code>Private_glm</code>, a regression model predicting the variable <code>Private</code>. Specifically,…</li>
</ol>
<ul>
<li>set the <code>form</code> argument to <code>Private ~ .</code>.</li>
<li>set the <code>data</code> argument to the training data <code>college_train</code>.</li>
<li>set the <code>method</code> argument to <code>&quot;glm&quot;</code>.</li>
<li>set the <code>trControl</code> argument to <code>ctrl_none</code>.</li>
</ul>
<pre class="r"><code># Fit regression model predicting Private
Private_glm &lt;- train(form = XXX ~ .,
                     data = XXX,
                     method = &quot;XXX&quot;,
                     trControl = XXX)</code></pre>
<pre class="r"><code># Fit regression model predicting private
Private_glm &lt;- train(form = Private ~ .,
                     data = college_train,
                     method = &quot;glm&quot;,
                     trControl = ctrl_none)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Explore the <code>Private_glm</code> object using the <code>summary()</code> function.</li>
</ol>
<pre class="r"><code># Explore the Private_glm object
summary(XXX)</code></pre>
<pre class="r"><code># Show summary information from the regression model
summary(Private_glm)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.9426  -0.0453   0.0272   0.1179   2.5261  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.25e+00   2.28e+00    0.55   0.5839    
Apps        -2.79e-04   2.71e-04   -1.03   0.3028    
Accept      -1.21e-03   5.48e-04   -2.20   0.0276 *  
Enroll       3.90e-03   1.40e-03    2.80   0.0052 ** 
Top10perc   -1.67e-02   3.82e-02   -0.44   0.6619    
Top25perc    3.09e-02   2.76e-02    1.12   0.2640    
F.Undergrad -4.14e-04   1.68e-04   -2.46   0.0140 *  
P.Undergrad -1.76e-04   2.05e-04   -0.86   0.3899    
Outstate     8.48e-04   1.55e-04    5.47  4.5e-08 ***
Room.Board   7.35e-04   3.60e-04    2.04   0.0410 *  
Books        3.42e-03   1.83e-03    1.87   0.0619 .  
Personal    -6.20e-04   3.88e-04   -1.60   0.1097    
PhD         -5.63e-02   3.73e-02   -1.51   0.1315    
Terminal    -6.57e-02   3.68e-02   -1.79   0.0739 .  
S.F.Ratio   -1.91e-01   7.46e-02   -2.56   0.0104 *  
perc.alumni  4.77e-02   2.79e-02    1.71   0.0876 .  
Expend       2.81e-05   1.53e-04    0.18   0.8542    
Grad.Rate    7.30e-03   1.48e-02    0.49   0.6220    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 609.16  on 499  degrees of freedom
Residual deviance: 144.29  on 482  degrees of freedom
AIC: 180.3

Number of Fisher Scoring iterations: 8</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Look at the results, how do you interpret the regression coefficients? Which features seem important in predicting whether a school is private or not?</li>
</ol>
<pre class="r"><code># Looking at the z statistics, Outstate, Enroll and S.F.Ratio (...) have quite large z-statistics</code></pre>
</div>
<div id="i---access-classification-model-accuracy" class="section level3">
<h3>I - Access classification model accuracy</h3>
<ol style="list-style-type: decimal">
<li>Now it’s time to save the model’s fitted values! Do this by running the following code to save the fitted values as <code>glm_fit</code>.</li>
</ol>
<pre class="r"><code># Get fitted values from the Private_glm object
glm_fit &lt;- predict(XXX)</code></pre>
<pre class="r"><code># Get fitted values from the Private_glm object
glm_fit &lt;- predict(Private_glm)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Plot the values of your <code>glm_fit</code> object - what are these values? Do they look reasonable?</li>
</ol>
<pre class="r"><code># Plot glm_fit
plot(glm_fit)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-64-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Now it’s time to calculate model accuracy. To do this, we will use a new function called <code>confusionMatrix()</code>. This function compares model predictions to a ‘reference’ (in our case, the criterion, and returns several summary statistics). In the code below, we’ll use <code>glm_fit</code> as the model predictions, and our already defined <code>criterion</code> vector as the reference (aka, truth). Specifically,…</li>
</ol>
<ul>
<li>set the <code>data</code> argument to your <code>glm_fit</code> values.</li>
<li>set the <code>reference</code> argument to the <code>criterion</code> values.</li>
</ul>
<pre class="r"><code># Show accuracy of glm_fit versus the true criterion values
confusionMatrix(data = XXX,      # This is the prediction!
                reference = XXX) # This is the truth!</code></pre>
<pre class="r"><code># Show accuracy of glm_fit versus the true values
confusionMatrix(data = glm_fit,        # This is the prediction!
                reference = criterion) # This is the truth!</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  133  13
       Yes  16 338
                                        
               Accuracy : 0.942         
                 95% CI : (0.918, 0.961)
    No Information Rate : 0.702         
    P-Value [Acc &gt; NIR] : &lt;2e-16        
                                        
                  Kappa : 0.861         
                                        
 Mcnemar&#39;s Test P-Value : 0.71          
                                        
            Sensitivity : 0.893         
            Specificity : 0.963         
         Pos Pred Value : 0.911         
         Neg Pred Value : 0.955         
             Prevalence : 0.298         
         Detection Rate : 0.266         
   Detection Prevalence : 0.292         
      Balanced Accuracy : 0.928         
                                        
       &#39;Positive&#39; Class : No            
                                        </code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Look at the results, what is the overall accuracy of the model? How do you interpret this?</li>
</ol>
<pre class="r"><code># The overall accuracy is 0.942. Across all cases, the model fits the true class values 94.2% of the time.</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>What is the sensitivity? How do you interpret this number?</li>
</ol>
<pre class="r"><code># The sensitivity is 0.893. Of those colleges that truly are private, the model fits are correct 89.3% of the time.</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>What is the positive predictive value? How do you interpret this number?</li>
</ol>
<pre class="r"><code># The PPV is 0.911. Of those colleges that are predicted to be private, 91.1% truly are private.</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>What is the specificity? How do you interpret this number?</li>
</ol>
<pre class="r"><code># The sensitivity is 0.963. Of those collges that truly are not private, the model fits are correct 96.3% of the time</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>What is the negative predictive value? How do you interpret this number?</li>
</ol>
<pre class="r"><code># The NPV is 0.955. Of those colleges that are predicted to be public, 95.5% truly are public.</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>To visualize the accuracy of your classification models, use the following code to create a bar plot.</li>
</ol>
<pre class="r"><code># Get overall accuracy from regression model
glm_accuracy &lt;- confusionMatrix(data =  glm_fit,  
                                reference = criterion)$overall[1]

# Combine results into one table
accuracy &lt;- tibble(Regression = glm_accuracy) %&gt;%
              gather(model, accuracy)


# Plot the results!
ggplot(accuracy, aes(x = model, y = accuracy, fill = model)) + 
  geom_bar(stat = &quot;identity&quot;) +
  labs(title = &quot;Is a college private or public?&quot;,
       subtitle = &quot;Fitting classification accuracy&quot;,
       y = &quot;Overall Accuracy&quot;) +
  ylim(c(0, 1)) +
  annotate(geom = &quot;label&quot;, 
           x = accuracy$model, 
           y = accuracy$accuracy, 
           label = round(accuracy$accuracy, 2))</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-72-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="z---challenges" class="section level3">
<h3>Z - Challenges</h3>
<ol style="list-style-type: decimal">
<li>Conduct a regression analysis predicting the percent of alumni who donate to the college (<code>perc.alumni</code>). How good can your regression model fit this criterion? Which variables seem to be important in predicting it?</li>
</ol>
<pre class="r"><code>mod &lt;- train(form = perc.alumni ~ .,
             data = college_train,
             method = &quot;glm&quot;,
             trControl = ctrl_none)

summary(mod)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-24.48   -6.05   -0.30    5.12   31.93  

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.22e+00   4.43e+00    0.95  0.34142    
PrivateYes   1.48e+00   1.54e+00    0.96  0.33690    
Apps        -7.58e-04   4.21e-04   -1.80  0.07231 .  
Accept      -1.66e-03   7.62e-04   -2.18  0.02984 *  
Enroll       6.88e-03   2.08e-03    3.31  0.00101 ** 
Top10perc    3.65e-02   6.30e-02    0.58  0.56276    
Top25perc    7.30e-02   4.93e-02    1.48  0.13894    
F.Undergrad -3.32e-04   3.38e-04   -0.98  0.32622    
P.Undergrad  5.27e-05   3.27e-04    0.16  0.87191    
Outstate     1.09e-03   2.19e-04    4.95    1e-06 ***
Room.Board  -1.75e-03   5.21e-04   -3.35  0.00088 ***
Books       -3.72e-04   2.99e-03   -0.12  0.90100    
Personal    -2.18e-03   7.23e-04   -3.01  0.00276 ** 
PhD         -4.28e-02   5.19e-02   -0.82  0.41045    
Terminal     1.40e-01   5.55e-02    2.53  0.01173 *  
S.F.Ratio   -2.55e-01   1.40e-01   -1.82  0.06873 .  
Expend       8.48e-05   1.49e-04    0.57  0.56928    
Grad.Rate    1.17e-01   3.28e-02    3.57  0.00039 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for gaussian family taken to be 82.5)

    Null deviance: 73707  on 499  degrees of freedom
Residual deviance: 39764  on 482  degrees of freedom
AIC: 3645

Number of Fisher Scoring iterations: 2</code></pre>
<pre class="r"><code>mod_predictions &lt;- predict(mod)
hist(mod_predictions)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-73-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>postResample(pred = mod_predictions,
             obs = college_train$perc.alumni)</code></pre>
<pre><code>    RMSE Rsquared      MAE 
   8.918    0.461    7.024 </code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Conduct a classification analysis predicting whether or not a school is ‘hot’ – where a ‘hot’ school is one that receives at least 10,000 applications (Hint: use the code below to create the <code>hot</code> variable).</li>
</ol>
<pre class="r"><code># Add a new factor criterion &#39;hot&#39; which indicates whether or not a schol receives at least 10,000 applications
college_train &lt;- college_train %&gt;%
  mutate(hot = factor(Apps &gt;= 10000))</code></pre>
<pre class="r"><code>mod_hot &lt;- train(form = hot ~ ., 
                 data = college_train,
                 method = &quot;glm&quot;,
                 trControl = ctrl_none)

summary(mod_hot)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
      Min         1Q     Median         3Q        Max  
-7.52e-05  -2.00e-08  -2.00e-08  -2.00e-08   6.52e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -6.34e+01   1.96e+05       0        1
PrivateYes  -4.40e+00   6.42e+04       0        1
Apps         1.96e-02   1.62e+01       0        1
Accept      -8.88e-03   1.83e+01       0        1
Enroll       1.17e-02   4.31e+01       0        1
Top10perc   -8.07e-01   2.33e+03       0        1
Top25perc    5.81e-01   2.14e+03       0        1
F.Undergrad  1.40e-03   7.34e+00       0        1
P.Undergrad -1.65e-05   3.31e+00       0        1
Outstate    -6.25e-04   1.15e+01       0        1
Room.Board   6.30e-03   1.24e+01       0        1
Books       -6.36e-02   1.09e+02       0        1
Personal    -3.81e-05   2.95e+01       0        1
PhD         -1.39e+00   2.10e+03       0        1
Terminal    -2.41e-01   4.59e+03       0        1
S.F.Ratio   -8.91e-01   3.22e+03       0        1
perc.alumni  1.86e-01   2.74e+03       0        1
Expend       6.00e-04   5.57e+00       0        1
Grad.Rate    3.72e-01   1.89e+03       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.5364e+02  on 499  degrees of freedom
Residual deviance: 4.4973e-08  on 481  degrees of freedom
AIC: 38

Number of Fisher Scoring iterations: 25</code></pre>
<pre class="r"><code>mod_predictions &lt;- predict(mod_hot)
plot(mod_predictions)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-75-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>confusionMatrix(data = mod_predictions,        # This is the prediction!
                reference = college_train$hot) # This is the truth!</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE   465    0
     TRUE      0   35
                                    
               Accuracy : 1         
                 95% CI : (0.993, 1)
    No Information Rate : 0.93      
    P-Value [Acc &gt; NIR] : &lt;2e-16    
                                    
                  Kappa : 1         
                                    
 Mcnemar&#39;s Test P-Value : NA        
                                    
            Sensitivity : 1.00      
            Specificity : 1.00      
         Pos Pred Value : 1.00      
         Neg Pred Value : 1.00      
             Prevalence : 0.93      
         Detection Rate : 0.93      
   Detection Prevalence : 0.93      
      Balanced Accuracy : 1.00      
                                    
       &#39;Positive&#39; Class : FALSE     
                                    </code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Did you notice anything strange in your model when doing the previous task? If you used all available predictors you will have gotten a warning that your model did not converge. That can happen if the maximum number of iterations (glm uses an iterative procedure when fitting the model) is reached. The default is a maximum of 25 iterations, see <code>?glm.control</code>. To fix it just add the following code in your <code>train()</code> function <code>control = list(maxit = 75)</code>, and run it again.</li>
</ol>
<pre class="r"><code>mod_hot &lt;- train(form = hot ~ ., 
                 data = college_train,
                 method = &quot;glm&quot;,
                 trControl = ctrl_none,
                 control = list(maxit = 75))

summary(mod_hot)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
      Min         1Q     Median         3Q        Max  
-6.30e-06  -2.10e-08  -2.10e-08  -2.10e-08   5.38e-06  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -7.72e+01   2.47e+06       0        1
PrivateYes  -6.17e+00   8.40e+05       0        1
Apps         2.43e-02   2.10e+02       0        1
Accept      -1.11e-02   2.25e+02       0        1
Enroll       1.48e-02   5.65e+02       0        1
Top10perc   -1.03e+00   3.24e+04       0        1
Top25perc    7.45e-01   2.91e+04       0        1
F.Undergrad  1.86e-03   9.21e+01       0        1
P.Undergrad -3.63e-05   4.00e+01       0        1
Outstate    -7.15e-04   1.54e+02       0        1
Room.Board   7.87e-03   1.50e+02       0        1
Books       -7.96e-02   1.33e+03       0        1
Personal    -1.79e-04   3.80e+02       0        1
PhD         -1.74e+00   2.58e+04       0        1
Terminal    -3.30e-01   5.87e+04       0        1
S.F.Ratio   -1.16e+00   4.00e+04       0        1
perc.alumni  2.05e-01   3.42e+04       0        1
Expend       7.89e-04   7.32e+01       0        1
Grad.Rate    4.91e-01   2.41e+04       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.5364e+02  on 499  degrees of freedom
Residual deviance: 3.0770e-10  on 481  degrees of freedom
AIC: 38

Number of Fisher Scoring iterations: 30</code></pre>
<pre class="r"><code>mod_predictions &lt;- predict(mod_hot)
plot(mod_predictions)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-76-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>confusionMatrix(data = mod_predictions,        # This is the prediction!
                reference = college_train$hot) # This is the truth!</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE   465    0
     TRUE      0   35
                                    
               Accuracy : 1         
                 95% CI : (0.993, 1)
    No Information Rate : 0.93      
    P-Value [Acc &gt; NIR] : &lt;2e-16    
                                    
                  Kappa : 1         
                                    
 Mcnemar&#39;s Test P-Value : NA        
                                    
            Sensitivity : 1.00      
            Specificity : 1.00      
         Pos Pred Value : 1.00      
         Neg Pred Value : 1.00      
             Prevalence : 0.93      
         Detection Rate : 0.93      
   Detection Prevalence : 0.93      
      Balanced Accuracy : 1.00      
                                    
       &#39;Positive&#39; Class : FALSE     
                                    </code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Now the model should have converged, but there is still another warning occurring: <code>glm.fit: fitted probabilities numerically 0 or 1 occurred</code>. This can happen if very strong predictors occur in the dataset (see <a href="http://www.bagualu.net/wordpress/wp-content/uploads/2015/10/Modern_Applied_Statistics_With_S.pdf">Venables &amp; Ripley, 2002</a>, p. 197). If you added all predictors (except again the college names), then this problem occurs because the <code>Apps</code> variable, used to create the criterion, was also part of the predictors (plus some other variables that highly correlate with <code>Apps</code>). Check the variable correlations (the code below will give you a matrix of bivariate correlations). You will learn an easier way of checking the correlations of variables in a later session.</li>
</ol>
<pre class="r"><code># get correlation matrix of numeric variables
cor(college_train[,sapply(college_train, is.numeric)])</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Now fit the model again but only select variables that are not directly related to the number of applications (here several solutions are possible, there is no clear-cut criterion about which variables to include and which to discard).</li>
</ol>
<pre class="r"><code>mod_hot &lt;- train(form = hot ~ . - Apps -Enroll -Accept - F.Undergrad,
                 data = college_train,
                 method = &quot;glm&quot;,
                 trControl = ctrl_none,
                 control = list(maxit = 75))

summary(mod_hot)</code></pre>
<pre><code>
Call:
NULL

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0603  -0.1783  -0.0609  -0.0177   3.0120  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.49e+01   4.42e+00   -3.37  0.00075 ***
PrivateYes  -4.85e+00   1.32e+00   -3.67  0.00025 ***
Top10perc    2.42e-02   2.60e-02    0.93  0.35115    
Top25perc    2.66e-02   2.73e-02    0.98  0.32940    
P.Undergrad  5.22e-04   1.62e-04    3.23  0.00124 ** 
Outstate     8.41e-05   1.30e-04    0.65  0.51670    
Room.Board   7.85e-04   3.33e-04    2.36  0.01826 *  
Books       -2.08e-03   2.33e-03   -0.90  0.37057    
Personal     2.77e-04   4.08e-04    0.68  0.49704    
PhD          1.65e-02   5.69e-02    0.29  0.77228    
Terminal     1.97e-02   6.10e-02    0.32  0.74625    
S.F.Ratio   -2.56e-03   8.09e-02   -0.03  0.97480    
perc.alumni -3.23e-02   3.21e-02   -1.01  0.31366    
Expend       2.68e-05   6.19e-05    0.43  0.66542    
Grad.Rate    6.40e-02   2.60e-02    2.47  0.01369 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 253.64  on 499  degrees of freedom
Residual deviance: 121.87  on 485  degrees of freedom
AIC: 151.9

Number of Fisher Scoring iterations: 8</code></pre>
<pre class="r"><code>mod_predictions &lt;- predict(mod_hot)
plot(mod_predictions)</code></pre>
<p><img src="Fitting_practical_files/figure-html/unnamed-chunk-78-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>confusionMatrix(data = mod_predictions,        # This is the prediction!
                reference = college_train$hot) # This is the truth!</code></pre>
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE   458   18
     TRUE      7   17
                                        
               Accuracy : 0.95          
                 95% CI : (0.927, 0.967)
    No Information Rate : 0.93          
    P-Value [Acc &gt; NIR] : 0.0429        
                                        
                  Kappa : 0.551         
                                        
 Mcnemar&#39;s Test P-Value : 0.0455        
                                        
            Sensitivity : 0.985         
            Specificity : 0.486         
         Pos Pred Value : 0.962         
         Neg Pred Value : 0.708         
             Prevalence : 0.930         
         Detection Rate : 0.916         
   Detection Prevalence : 0.952         
      Balanced Accuracy : 0.735         
                                        
       &#39;Positive&#39; Class : FALSE         
                                        </code></pre>
</div>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<pre class="r"><code># Fitting and evaluating a regression model ------------------------------------

# Step 0: Load packages-----------
library(tidyverse)    # Load tidyverse for dplyr and tidyr
library(caret)        # For ML mastery 

# Step 1: Load and Clean, and Explore Training data ----------------------

# I&#39;ll use the mpg dataset from the dplyr package in this example
#  no need to load an external dataset
data_train &lt;- read_csv(&quot;1_Data/mpg_train.csv&quot;)

# Convert all characters to factor
#  Some ML models require factors
data_train &lt;- data_train %&gt;%
  mutate_if(is.character, factor)

# Explore training data
data_train        # Print the dataset
View(data_train)  # Open in a new spreadsheet-like window 
dim(data_train)   # Print dimensions
names(data_train) # Print the names

# Step 2: Define training control parameters -------------

# In this case, I will set method = &quot;none&quot; to fit to 
#  the entire dataset without any fancy methods
#  such as cross-validation
train_control &lt;- trainControl(method = &quot;none&quot;) 

# Step 3: Train model: -----------------------------
#   Criterion: hwy
#   Features: year, cyl, displ, trans

# Regression
hwy_glm &lt;- train(form = hwy ~ year + cyl + displ + trans,
                 data = data_train,
                 method = &quot;glm&quot;,
                 trControl = train_control)

# Look at summary information
summary(hwy_glm)

# Step 4: Access fit ------------------------------

# Save fitted values
glm_fit &lt;- predict(hwy_glm)

# Define data_train$hwy as the true criterion
criterion &lt;- data_train$hwy

# Regression Fitting Accuracy
postResample(pred = glm_fit, 
             obs = criterion)

#     RMSE Rsquared      MAE 
# 3.246182 0.678465 2.501346 

# On average, the model fits are 2.8 away from the true
#  criterion values

# Step 5: Visualise Accuracy -------------------------

# Tidy competition results
accuracy &lt;- tibble(criterion = criterion,
                   Regression = glm_fit) %&gt;%
               gather(model, prediction, -criterion) %&gt;%
  
  # Add error measures
  mutate(se = prediction - criterion,
         ae = abs(prediction - criterion))


# Calculate summaries
accuracy_agg &lt;- accuracy %&gt;%
                  group_by(model) %&gt;%
                  summarise(mae = mean(ae))   # Calculate MAE (mean absolute error)

# Plot A) Scatterplot of criterion versus predictions
ggplot(data = accuracy,
       aes(x = criterion, y = prediction, col = model)) +
  geom_point(alpha = .2) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = &quot;Predicting mpg$hwy&quot;,
       subtitle = &quot;Black line indicates perfect performance&quot;)

# Plot B) Violin plot of absolute errors
ggplot(data = accuracy, 
       aes(x = model, y = ae, fill = model)) + 
  geom_violin() + 
  geom_jitter(width = .05, alpha = .2) +
  labs(title = &quot;Distributions of Fitting Absolute Errors&quot;,
       subtitle = &quot;Numbers indicate means&quot;,
       x = &quot;Model&quot;,
       y = &quot;Absolute Error&quot;) +
  guides(fill = FALSE) +
  annotate(geom = &quot;label&quot;, 
           x = accuracy_agg$model, 
           y = accuracy_agg$mae, 
           label = round(accuracy_agg$mae, 2))</code></pre>
</div>
<div id="datasets" class="section level2">
<h2>Datasets</h2>
<table>
<thead>
<tr class="header">
<th align="left">File</th>
<th align="left">Rows</th>
<th align="left">Columns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="https://raw.githubusercontent.com/therbootcamp/ML_2019Oct/master/1_Data/college_train.csv?token=AGKBX5SLEV3PLWUVQ4NCUB2427V36">college_train.csv</a></td>
<td align="left">1000</td>
<td align="left">21</td>
</tr>
</tbody>
</table>
<ul>
<li>The <code>college_train</code> data are taken from the <code>College</code> dataset in the <code>ISLR</code> package. They contain statistics for a large number of US Colleges from the 1995 issue of US News and World Report.</li>
</ul>
<div id="variable-description-of-college_train" class="section level4">
<h4>Variable description of <code>college_train</code></h4>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>Private</code></td>
<td align="left">A factor with levels No and Yes indicating private or public university.</td>
</tr>
<tr class="even">
<td align="left"><code>Apps</code></td>
<td align="left">Number of applications received.</td>
</tr>
<tr class="odd">
<td align="left"><code>Accept</code></td>
<td align="left">Number of applications accepted.</td>
</tr>
<tr class="even">
<td align="left"><code>Enroll</code></td>
<td align="left">Number of new students enrolled.</td>
</tr>
<tr class="odd">
<td align="left"><code>Top10perc</code></td>
<td align="left">Pct. new students from top 10% of H.S. class.</td>
</tr>
<tr class="even">
<td align="left"><code>Top25perc</code></td>
<td align="left">Pct. new students from top 25% of H.S. class.</td>
</tr>
<tr class="odd">
<td align="left"><code>F.Undergrad</code></td>
<td align="left">Number of fulltime undergraduates.</td>
</tr>
<tr class="even">
<td align="left"><code>P.Undergrad</code></td>
<td align="left">Number of parttime undergraduates.</td>
</tr>
<tr class="odd">
<td align="left"><code>Outstate</code></td>
<td align="left">Out-of-state tuition.</td>
</tr>
<tr class="even">
<td align="left"><code>Room.Board</code></td>
<td align="left">Room and board costs.</td>
</tr>
<tr class="odd">
<td align="left"><code>Books</code></td>
<td align="left">Estimated book costs.</td>
</tr>
<tr class="even">
<td align="left"><code>Personal</code></td>
<td align="left">Estimated personal spending.</td>
</tr>
<tr class="odd">
<td align="left"><code>PhD</code></td>
<td align="left">Pct. of faculty with Ph.D.’s.</td>
</tr>
<tr class="even">
<td align="left"><code>Terminal</code></td>
<td align="left">Pct. of faculty with terminal degree.</td>
</tr>
<tr class="odd">
<td align="left"><code>S.F.Ratio</code></td>
<td align="left">Student/faculty ratio.</td>
</tr>
<tr class="even">
<td align="left"><code>perc.alumni</code></td>
<td align="left">Pct. alumni who donate.</td>
</tr>
<tr class="odd">
<td align="left"><code>Expend</code></td>
<td align="left">Instructional expenditure per student.</td>
</tr>
<tr class="even">
<td align="left"><code>Grad.Rate</code></td>
<td align="left">Graduation rate.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="functions" class="section level2">
<h2>Functions</h2>
<div id="packages" class="section level3">
<h3>Packages</h3>
<table>
<thead>
<tr class="header">
<th align="left">Package</th>
<th align="left">Installation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>tidyverse</code></td>
<td align="left"><code>install.packages(&quot;tidyverse&quot;)</code></td>
</tr>
<tr class="even">
<td align="left"><code>caret</code></td>
<td align="left"><code>install.packages(&quot;caret&quot;)</code></td>
</tr>
</tbody>
</table>
</div>
<div id="functions-1" class="section level3">
<h3>Functions</h3>
<table>
<colgroup>
<col width="7%" />
<col width="12%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Function</th>
<th align="left">Package</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>trainControl()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Define modelling control parameters</td>
</tr>
<tr class="even">
<td align="left"><code>train()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Train a model</td>
</tr>
<tr class="odd">
<td align="left"><code>predict(object, newdata)</code></td>
<td align="left"><code>base</code></td>
<td align="left">Predict the criterion values of <code>newdata</code> based on <code>object</code></td>
</tr>
<tr class="even">
<td align="left"><code>postResample()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in regression tasks</td>
</tr>
<tr class="odd">
<td align="left"><code>confusionMatrix()</code></td>
<td align="left"><code>caret</code></td>
<td align="left">Calculate aggregate model performance in classification tasks</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="resources" class="section level2">
<h2>Resources</h2>
<div id="cheatsheet" class="section level3">
<h3>Cheatsheet</h3>
<figure>
<center>
<a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf"> <img src="https://www.rstudio.com/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="Trulli" style="width:70%"></a><br> <font style="font-size:10px"> from <a href= "https://github.com/rstudio/cheatsheets/raw/master/caret.pdf</figcaption">github.com/rstudio</a></font>
</figure>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
