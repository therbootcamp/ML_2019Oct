---
title: "Features"
author: "Machine Learning with R<br>
  <a href='https://therbootcamp.github.io'>
    Basel R Bootcamp
  </a>
  <br>
  <a href='https://therbootcamp.github.io/ML_2019May/'>
    <i class='fas fa-clock' style='font-size:.9em;'></i>
  </a>&#8239; 
  <a href='https://therbootcamp.github.io'>
    <i class='fas fa-home' style='font-size:.9em;' ></i>
  </a>&#8239;
  <a href='mailto:therbootcamp@gmail.com'>
    <i class='fas fa-envelope' style='font-size: .9em;'></i>
  </a>&#8239;
  <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
    <i class='fab fa-linkedin' style='font-size: .9em;'></i>
  </a>"
date: "May 2019"
output:
  xaringan::moon_reader:
    css: ["default", "baselrbootcamp.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---

layout: true

<div class="my-footer">
  <span style="text-align:center">
    <span> 
      <img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png" height=14 style="vertical-align: middle"/>
    </span>
    <a href="https://therbootcamp.github.io/">
      <span style="padding-left:82px"> 
        <font color="#7E7E7E">
          www.therbootcamp.com
        </font>
      </span>
    </a>
    <a href="https://therbootcamp.github.io/">
      <font color="#7E7E7E">
       Machine Learning with R | May 2019
      </font>
    </a>
    </span>
  </div> 

---

```{r, eval = TRUE, echo = FALSE, warning=F,message=F}
require(caret)
require(tidyverse)
# Code to knit slides
#bas = read_csv('1_Data/baselers.csv')
bas = read_csv('../../1_Data/baselers.csv')

print2 <- function(x, nlines=10,...)
   cat(head(capture.output(print(x,...)), nlines), sep="\n")

sel = apply(bas, 1, function(x) any(is.na(unlist(x))))
bas = bas[!sel,]

bas = bas %>% 
  mutate_if(is.character, as.factor)

preprocesssing = preProcess(bas)
bas = predict(preprocesssing, bas)


bas <- bas %>%
  sample_n(1000)

bas <- bas %>% select_if(is.numeric)
baselers = bas

fitControl_cv <- trainControl(
  method = "repeatedcv",
  number = 1,
  repeats = 1)

train_index = createDataPartition(bas$income, p = .8, list = FALSE)
bas_train = bas %>% slice(train_index)
bas_test = bas %>% slice(-train_index)

income_lm = train(income ~ . - id, 
           method = 'lm', 
           data = bas_train)

income_lm_short = train(income ~ age + food + alcohol + happiness + fitness + datause + tattoos + weight + children + fitness + height, 
           method = 'lm', 
           data = bas_train)

```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
```

```{r, echo = FALSE ,message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, 
                      message = FALSE, warning = FALSE, fig.align = 'center', dpi = 200)
library(tidyverse)
#library(baselers)
library(ggthemes)
```



.pull-left45[

# Feature issues
<br>
<b>Too many features</b>

- Curse of dimensionality
- Feature importance

<b>Wrong features</b>

- Feature scaling
- Feature correlation
- Feature quality

<b>Create new features</b>

- Feature engineering

]

.pull-right45[

<br><br>

<p align="center">
<img src="image/dumbdata.png" height = 500px><br>
<font style="font-size:10px">from <a href="https://xkcd.com/1838/">xkcd.com</a></font>
</p>

]

---

# Curse of dimensionality

.pull-left35[

As the number of features grows...

<high>Performance</high> - the amount of data that needs to generalize accurately grows exponentially.

<high>Efficiency</high> - the amount of computations grows (how much depends on the model).

<high>Redundancy</high> - the amount of redundancy grows (how much depends on the model). 

&#8594; <high>Small set of good predictors<high>

]

.pull-right6[

<br>

<p align="center">
<img src="image/cod.png"><br>
<font style="font-size:10px">from <a href="https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335?gi=6e6735e00188">medium.freecodecamp.org</a></font>
</p>



]

---

# How to reduce dimensionality?

.pull-left45[

<b>3 ways</b>

1 - Reduce variables <high>manually</high> based on statistical or intuitive considerations.

2 - Reduce variables <high>automatically</high> using the right ML algorithms, e.g., `random forests` or `lasso regression`, or feature selection algorithms, e.g., `recursive feature selection`.

3 - Compress variables using <high>dimensionality reduction algorithms</high>, such as `principal component analysis` (PCA).

]

.pull-right5[

<p align = "center">
<img src="image/highd.jpeg" height=350>
<font style="font-size:10px">from <a href="">Interstellar</a></font>
</p>

]

---

# Feature importance

.pull-left4[

<high>Feature importance</high> characterizes how much a feature contributes to the fitting/prediction performance. 



The metric is <high>model specific </high>, but typically <high>normalized</high> to `[0, 100]`.

<u>Strategies</u>
- Single variable prediction (e.g., using `LOESS`, `ROC`) 
- Accuracy loss from scrambling
- `random forests` importance
- etc.  
]

.pull-right5[
```{r, eval = FALSE}
# plot variable importance for lm(income ~ .)
plot(varImp(income_lm))
```

```{r, echo = FALSE, fig.height=5.2, fig.width=8}
plot(varImp(income_lm),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```

]

---

# `varImp()`

.pull-left45[
`varImp()` <high>automatically selects appropriate measure</high> of variable importance for a given algorithm. 

```{r eval = FALSE}
varImp(income_lm)
```

```{r echo = F}
print2(varImp(income_lm_short), 11)
```

]

.pull-right5[
```{r, eval = FALSE}
# plot variable importance for lm(income ~ .)
plot(varImp(income_lm))
```

```{r, echo = FALSE, fig.height=5.2, fig.width=8}
plot(varImp(income_lm),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```

]

---

.pull-left35[

# Recursive feature selection `rfe()`

Find the <high>best number of `n` predictors</high>, with `n` drawn from specified candidate sets `N`, e.g., `N = [2,3,5,10]`. 

<u>Algorithm</u>
1. <high>Resample</high> and split data<br2>
2. Identify <high>best `n` predictors</high> and their prediction performance<br2>
3. <high>Aggregate performance</high> and select best `n` and the accordingly best predictors

]

.pull-right55[

<br><br>

```{r, eval = F}
# Run feature elimination
rfe(x = ..., y = ..., 
    sizes = c(3,4,5,10), # feature set sizes
    rfeControl = rfeControl(functions = lmFuncs))
```

```{r, echo = F}
# Run feature elimination
out = rfe(x = bas_train %>% select(-income), 
    y = bas_train$income,
    sizes = c(3,4,5,10),   # Features set sizes should be considered
    rfeControl = rfeControl(
      functions = lmFuncs,
      verbose = FALSE))
print2(out, 20)
```

]

---

# Dimensionality reduction using `PCA`

.pull-left45[

The go-to algorithm for dimensionality reduction is <high>principal component analysis</high> (PCA). 

PCA is an <high>unsupervised</high>, regression-based algorithm that re-represents the data in a <high>new feature space</high>.  

The new features aka principal components are greedy. <high>Skimming off the best components</high> results in a small number of features that <high>preserve the original features</high> as well as possible.

]


.pull-right45[

<p align = "center">
<img src="image/pca.png" height=350>
<font style="font-size:10px">from <a href="https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used
">blog.umetrics.com</a></font>
</p>


]

---

# Using `PCA`

.pull-left45[

```{r fig.height=5.1, fig.width=8, eval = F}
# train model WITHOUT PCA preprocessing
model = train(income ~ ., method = 'lm', 
           data = bas_train)

plot(varImp(model))
```

```{r fig.height=5.1, fig.width=8, echo = F}
# train model WITHOUT PCA preprocessing
model = train(income ~ . -id, method = 'lm', 
              data = bas_train)

plot(varImp(model, scale=F),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```


]

.pull-right45[

```{r fig.height=5.1, fig.width=8, eval = F}
# train model WITH PCA preprocessing
model = train(income ~ ., method = 'lm', 
              data = bas_train,
              preProc = c('pca'))
plot(varImp(model))
```

```{r fig.height=5.1, fig.width=8, echo = F}
# train model WITH PCA preprocessing
model = train(income ~ . -id, method = 'lm', 
              data = bas_train,
              preProc = c('pca'),
              trControl = trainControl(preProcOptions = list(thresh = 0.75)))
plot(varImp(model, scale=F),col='#EA4B68',cex=1.3,scales = list(cex=1.3,lineheight=1.3),xlab=list(cex=1.5))
```

]

---

# Other, easy feature problems

.pull-left45[

### Multi-collinearity

Multi-collinearity, <high>high feature correlations</high>, mean that there is redundancy in the data, which can lead to <high>less stable fits</high>, <high>uninterpretable variable importances</high>, and <high>worse predictions</high>.

```{r}
# identify redundant variables
findCorrelation(cor(baselers))

# remove from data
remove <- findCorrelation(cor(baselers))
baselers <- baselers %>%
  select(-remove)

```

]

.pull-right45[

### Unequal & low variance 

Unequal variance <high>breaks regularization</high> (L1, L2) and renders estimates difficult to interpret.

```{r, eval = F}
# standardize and center variables
train(..., preProc("center", "scale"))
```

Low variance variables add parameters, but <high>can hardly contribute to prediction</high> and are, thus, also redundant.

```{r}
# identify low variance variables
nearZeroVar(baselers)
```

]


---

# Difficult feature problems

<br>
.pull-left25[

1 - <b>Trivial features</b> 

Successful prediction not necessarily implies that a meaningful pattern has been detected.

<br>
2 - <b>Missing features</b>

Some problems are hard, requiring the engineering of new features. 

]


.pull-right65[
<br>

<p align = "center">
<img src="image/here_to_help.png"><br>
<font style="font-size:10px">from <a href="https://xkcd.com/1831/">xkcd.com</a></font>
</p>


]

---

# Trivial features

.pull-left3[

<u><a href="https://www.gwern.net/Tanks">An urban myth?!</a></u>

"The Army trained a program to differentiate American tanks from Russian tanks with 100% accuracy. Only later did analysts realize that the American tanks had been photographed on a sunny day and the Russian tanks had been photographed on a cloudy day. The computer had learned to detect brightness."<br><br>
New York Times <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html" style="font-size:8px">[Full text]</a>

]

.pull-right6[

<p align = "center">
<img src="image/tank.jpg">
<font style="font-size:10px">from <a href="https://en.wikipedia.org/wiki/British_heavy_tanks_of_World_War_I#/media/File:Mark_I_series_tank.jpg">wikipedia.org</a></font>
</p>





]


---

# Trivial features


In 2012, Nate Silver was praised to have correctly predicted the outcomes of the presidential election in 50 states after having correctly predicted 49 states in 2008. <high>But how much of a challenge was that?</high>

.pull-left5[

<p align = "center">
<img src="image/elect2008.png" height = 360px><br>
<font style="font-size:10px">from <a href="https://www.vox.com/policy-and-politics/2016/11/8/13563106/election-map-historical-vote">vox.com</a></font>
</p>

]

.pull-right5[

<p align = "center">
<img src="image/elect2012.png" height = 360px><br>
<font style="font-size:10px">from <a href="https://www.vox.com/policy-and-politics/2016/11/8/13563106/election-map-historical-vote">vox.com</a></font>
</p>



]

---

# (Always!) missing features

<i>"…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used."</i>

Pedro Domingos

<i>"The algorithms we used are very standard for Kagglers. […] We spent most of our efforts in feature engineering. [...] We were also very careful to discard features likely to expose us to the risk of over-fitting our model."</i>

Xavier Conort

<i>"Coming up with features is difficult, time-consuming, requires expert knowledge. "Applied machine learning" is basically feature engineering."</i>

Andrew Ng

---

# Feature engineering

.pull-left45[

<i>“Feature engineering is the process of <high>transforming raw data</high> into features that <high>better represent the underlying problem</high> to the predictive models, resulting in improved model accuracy on unseen data.”</i>

Jason Brownlee

<i>"...while avoiding the <high>curse of dimensionality</high>."</i>

duw

<u>Feature engineering involves</u>

- <b>Transformations</b>
- <b>Interactions</b>
- <b>New features</b>

]

.pull-right45[

<p align = "center">
<img src="image/albert.jpeg"><br>
<font style="font-size:10px">from <a href="http://www.open.edu/openlearncreate/mod/oucontent/view.php?id=80245&section=1">open.edu</a></font>
</p>


]

---

class: middle, center

<h1><a href="https://therbootcamp.github.io/ML_2019May/_sessions/Features/Features_practical.html">Practical</a></h1>

